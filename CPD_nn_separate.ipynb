{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-eORkj0HEIW",
        "outputId": "ef4eccb0-d29d-42b4-def1-b25ed07f8fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "  print('GPU')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print('CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QZtxEYmXJbqw"
      },
      "outputs": [],
      "source": [
        "def parse_args():\n",
        "  parser = argparse.ArgumentParser()\n",
        "\n",
        "  parser.add_argument('--directed', default=True)\n",
        "  parser.add_argument('--time_step', default=40)\n",
        "  parser.add_argument('--CP_true', default=[10,20,30])\n",
        "\n",
        "  parser.add_argument('--latent_dim', default=50)\n",
        "  parser.add_argument('--w_dim', default=50)\n",
        "  parser.add_argument('--num_node', default=30)\n",
        "\n",
        "  parser.add_argument('--shared_layer', default=[128, 256, 512])\n",
        "  parser.add_argument('--output_layer', default=[64, 128, 256])\n",
        "  parser.add_argument('--num_samples', default=100)\n",
        "  parser.add_argument('--langevin_K', default=50)\n",
        "  parser.add_argument('--langevin_s', default=0.5)\n",
        "\n",
        "  parser.add_argument('--decoder_lr', default=0.01)\n",
        "  parser.add_argument('--decay_rate', default=0.01)\n",
        "  parser.add_argument('--penalty', default=10)\n",
        "  parser.add_argument('--mu_lr', default=0.01)\n",
        "  parser.add_argument('--epoch',default=100)\n",
        "  parser.add_argument('-f', required=False) # needed in Colab\n",
        "\n",
        "  return parser.parse_args()\n",
        "\n",
        "###################\n",
        "args = parse_args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li26wVrvf33i"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "rho = 0.0\n",
        "n = args.num_node\n",
        "num_time = args.time_step\n",
        "K = 3\n",
        "v = args.CP_true\n",
        "data = torch.zeros(num_time, n, n)\n",
        "sum_holder =[]\n",
        "\n",
        "\n",
        "for t in range(num_time):\n",
        "    if t == 0 or t == v[1]:\n",
        "        P = torch.full((n, n), 0.3)\n",
        "        P[:n // K, :n // K] = 0.5\n",
        "        P[n // K:2 * (n // K), n // K:2 * (n // K)] = 0.5\n",
        "        P[2 * (n // K):n, 2 * (n // K):n] = 0.5\n",
        "        torch.diagonal(P).zero_()\n",
        "        A = torch.bernoulli(P)\n",
        "\n",
        "    if t == v[0] or t == v[2]:\n",
        "        Q = torch.full((n, n), 0.2)\n",
        "        Q[:n // K, :n // K] = 0.45\n",
        "        Q[n // K:2 * (n // K), n // K:2 * (n // K)] = 0.45\n",
        "        Q[2 * (n // K):n, 2 * (n // K):n] = 0.45\n",
        "        torch.diagonal(Q).zero_()\n",
        "        A = torch.bernoulli(Q)\n",
        "\n",
        "    if (t > 0 and t < v[0]) or (t > v[1] and t < v[2]):\n",
        "        aux1 = (1 - P) * rho + P\n",
        "        aux2 = P * (1 - rho)\n",
        "        aux1 = torch.bernoulli(aux1)\n",
        "        aux2 = torch.bernoulli(aux2)\n",
        "        A = aux1 * A + aux2 * (1 - A)\n",
        "\n",
        "    if (t > v[0] and t < v[1]) or (t > v[2] and t <= num_time):\n",
        "        aux1 = (1 - Q) * rho + Q\n",
        "        aux2 = Q * (1 - rho)\n",
        "        aux1 = torch.bernoulli(aux1)\n",
        "        aux2 = torch.bernoulli(aux2)\n",
        "        A = aux1 * A + aux2 * (1 - A)\n",
        "\n",
        "    torch.diagonal(A).zero_()\n",
        "    #A = A + torch.eye(args.num_node)\n",
        "    data[t,:,:] = A.clone()\n",
        "    sum_holder.append(torch.sum(A))\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "plt.plot(np.arange(0, args.time_step), sum_holder)  \n",
        "plt.show()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ0_lpBEh1ZM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "plt.imshow(data[9,:,:].numpy(),cmap=\"Greys\")\n",
        "plt.show()\n",
        "plt.imshow(data[10,:,:].numpy(),cmap=\"Greys\")\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(data[19,:,:].numpy(),cmap=\"Greys\")\n",
        "plt.show()\n",
        "plt.imshow(data[20,:,:].numpy(),cmap=\"Greys\")\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(data[29,:,:].numpy(),cmap=\"Greys\")\n",
        "plt.show()\n",
        "plt.imshow(data[30,:,:].numpy(),cmap=\"Greys\")\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Wi7WDaepHL7J"
      },
      "outputs": [],
      "source": [
        "class CPD(nn.Module):\n",
        "  def __init__(self, args):\n",
        "    super(CPD, self).__init__()\n",
        "    \n",
        "    #self.l1 = nn.Linear( args.latent_dim,      args.shared_layer[0] )\n",
        "    #self.l2 = nn.Linear( args.shared_layer[0], args.shared_layer[1] )\n",
        "    #self.l3 = nn.Linear( args.shared_layer[1], args.shared_layer[2] )\n",
        "    #self.l4 = nn.Linear( args.shared_layer[2], args.num_node * args.num_node )\n",
        "\n",
        "    self.l1 = nn.Linear( args.latent_dim, args.output_layer[0] )\n",
        "    self.left1 = nn.Linear( args.output_layer[0], args.output_layer[1] ) \n",
        "    self.left2 = nn.Linear( args.output_layer[1], args.num_node * args.w_dim ) \n",
        "    self.middle1 = nn.Linear( args.output_layer[0], args.output_layer[1] ) \n",
        "    self.middle2 = nn.Linear( args.output_layer[1], args.w_dim * args.w_dim ) \n",
        "    self.right1 = nn.Linear( args.output_layer[0], args.output_layer[1] ) \n",
        "    self.right2 = nn.Linear( args.output_layer[1], args.num_node * args.w_dim ) \n",
        "\n",
        "  def forward(self, z):\n",
        "    #output = self.l1(z).tanh()\n",
        "    #output = self.l2(output).tanh()\n",
        "    #output = self.l3(output).tanh()\n",
        "    #output = self.l4(output).sigmoid()\n",
        "    \n",
        "    output = self.l1(z).tanh()\n",
        "    w_left = self.left1(output).tanh()\n",
        "    w_left = self.left2(w_left).tanh()\n",
        "    w_middle = self.middle1(output).tanh()\n",
        "    w_middle = self.middle2(w_middle).tanh()\n",
        "    w_right = self.right1(output).tanh() \n",
        "    w_right = self.right2(w_right).tanh()\n",
        "\n",
        "    w_left = w_left.reshape(args.num_samples, args.num_node, args.w_dim)\n",
        "    w_middle = w_middle.reshape(args.num_samples, args.w_dim, args.w_dim)\n",
        "    w_right = w_right.reshape(args.num_samples, args.num_node, args.w_dim)\n",
        "    output = torch.bmm(torch.bmm(w_left, w_middle),torch.transpose(w_right, 1, 2)).sigmoid() # n by n\n",
        "\n",
        "    return output\n",
        "    \n",
        "  def infer_z(self, z, adj_gt_vec, mu_t):\n",
        "    '''\n",
        "    z: m by d\n",
        "    adj_gt_vec: m*n*n (with repetition)\n",
        "    mu_t_mat: m by d (with repetition)\n",
        "    '''\n",
        "\n",
        "    criterion = nn.BCELoss(reduction='sum') # take the sum ???? divided by m\n",
        "\n",
        "    for k in range(args.langevin_K):\n",
        "\n",
        "      z = z.detach().clone()\n",
        "      z.requires_grad = True\n",
        "      assert z.grad is None\n",
        "\n",
        "      adj_prob = self.forward(z) # m by (n*n)\n",
        "      nll = criterion( adj_prob.view(-1), adj_gt_vec ) # both are m*n*n\n",
        "      z_grad_nll = torch.autograd.grad(nll, z)[0] # m by d \n",
        "\n",
        "      z = z - args.langevin_s * (z_grad_nll + (z-mu_t)) + \\\n",
        "          torch.sqrt(2*torch.tensor(args.langevin_s)) * torch.randn(args.num_samples, args.latent_dim).to(device)\n",
        "\n",
        "    z = z.detach().clone()\n",
        "    return z\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.to(device)\n",
        "T = data.shape[0] # data is T by n by n\n",
        "mu = torch.zeros(T, args.latent_dim).to(device) # initialize as random, divided by norm of row diff (cannot be identical)\n",
        "\n",
        "\n",
        "mu_old = mu.detach().clone()\n",
        "loss_holder = []\n",
        "\n",
        "model = CPD(args).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.decoder_lr) # weight_decay=args.decay_rate\n",
        "criterion = nn.BCELoss(reduction='sum') # sum for expectation, later divided by m\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr-KDWQtNvkp",
        "outputId": "f9d64d75-290a-40b5-81e5-793a16f40880"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CPD(\n",
              "  (l1): Linear(in_features=50, out_features=64, bias=True)\n",
              "  (left1): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (left2): Linear(in_features=128, out_features=1500, bias=True)\n",
              "  (middle1): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (middle2): Linear(in_features=128, out_features=2500, bias=True)\n",
              "  (right1): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (right2): Linear(in_features=128, out_features=1500, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvl7mHMLdD-0"
      },
      "outputs": [],
      "source": [
        "for learning_iter in range(args.epoch):\n",
        "\n",
        "  loss = 0.0\n",
        "\n",
        "  for t in range(T):\n",
        "    # transformation\n",
        "    mu_t = mu[t,:].clone() # d\n",
        "    #mu_t_mat = mu_t.repeat(args.num_samples, 1) # m by d (with repetition)\n",
        "\n",
        "    adj_gt = data[t,:,:].clone() # n by n\n",
        "    adj_gt_vec = adj_gt.view(-1).repeat(args.num_samples) # m*n*n (with repetition)\n",
        "    \n",
        "    # sample from posterior\n",
        "    init_z = torch.randn(args.num_samples, args.latent_dim).to(device) # m by d, starts from N(0,1)\n",
        "    sampled_z = model.infer_z(init_z, adj_gt_vec, mu_t) # m by d, m samples of z from langevin\n",
        "\n",
        "    adj_prob = model(sampled_z) # m by (n*n) # m samples of adj_prob from the decoder\n",
        "    loss += criterion(adj_prob.view(-1), adj_gt_vec) / args.num_samples  # both are m*n*n\n",
        "\n",
        "  #loss /= args.time_step * args.num_samples # sum divided by m * T\n",
        "  loss_holder.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    # update decoder (after all time t)\n",
        "  for param in model.parameters():\n",
        "    param.grad = None\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if (learning_iter+1) % 10 == 0:\n",
        "    print('\\n')\n",
        "    print('learning iter =', learning_iter)\n",
        "    print('decoder loss =',loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mu = torch.randn(T, args.latent_dim).to(device) # cannot initialize as zero\n",
        "print(mu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O53yXKBhu_sr",
        "outputId": "2250bec0-dce1-4c4f-cf65-514b08b9e387"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.7137,  0.9684,  0.2790,  ..., -1.1308, -0.6657,  1.1658],\n",
            "        [ 0.1002, -1.1241, -0.6680,  ...,  0.1527,  0.6374, -0.0591],\n",
            "        [ 1.1298, -0.0846, -2.2610,  ...,  0.4736,  0.5762,  0.8806],\n",
            "        ...,\n",
            "        [-0.0831, -0.9283, -0.5310,  ..., -0.4127, -0.3400, -0.6185],\n",
            "        [ 0.6562,  0.3045, -0.0962,  ...,  0.8002, -0.9390, -1.0531],\n",
            "        [-1.2125, -0.5826, -0.7662,  ..., -1.0184, -0.5725,  1.1501]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for learning_iter in range(100):\n",
        "  \n",
        "  for t in range(T):\n",
        "    # transformation\n",
        "    mu_t = mu[t,:].clone() # d\n",
        "    #mu_t_mat = mu_t.repeat(args.num_samples, 1) # m by d (with repetition)\n",
        "\n",
        "    adj_gt = data[t,:,:].clone() # n by n\n",
        "    adj_gt_vec = adj_gt.view(-1).repeat(args.num_samples) # m*n*n (with repetition)\n",
        "    \n",
        "    # sample from posterior\n",
        "    init_z = torch.randn(args.num_samples, args.latent_dim).to(device) # m by d, starts from N(0,1)\n",
        "    sampled_z = model.infer_z(init_z, adj_gt_vec, mu_t) # m by d, m samples of z from langevin\n",
        "\n",
        "    if t == 0:  \n",
        "      grad_mu_t = -(sampled_z - mu_t).mean(dim=0) - args.penalty * (1/torch.norm(mu[1,:] - mu[0,:],p=2)) * (mu[1,:] - mu[0,:])\n",
        "    elif t == T-1:\n",
        "      grad_mu_t = -(sampled_z - mu_t).mean(dim=0) + args.penalty * (1/torch.norm(mu[t,:] - mu[t-1,:],p=2)) * (mu[t,:] - mu[t-1,:])\n",
        "    else:\n",
        "      grad_mu_t = -(sampled_z - mu_t).mean(dim=0) - args.penalty * (1/torch.norm(mu[t+1,:] - mu[t,:],p=2)) * (mu[t+1,:] - mu[t,:]) \\\n",
        "                                                  + args.penalty * (1/torch.norm(mu[t,:] - mu[t-1,:],p=2)) * (mu[t,:] - mu[t-1,:]) \n",
        "\n",
        "    mu = mu.detach().clone()\n",
        "    mu[t,:] -=  0.01 * grad_mu_t # gradient descent # args.mu_lr\n",
        "\n",
        "  if (learning_iter+1) % 10 == 0:\n",
        "    print('\\n')\n",
        "    print('learning iter =', learning_iter)\n",
        "    print('mu residual =',torch.mean((mu-mu_old)**2))\n",
        "    print('mu relative difference =',torch.norm(mu-mu_old,  p='fro') / torch.norm(mu_old,  p='fro'))\n",
        "    mu_old = mu.detach().clone()\n",
        "\n",
        "    signal = torch.norm(mu, p=2, dim=1)**2 \n",
        "    signal = signal.cpu().detach().numpy()\n",
        "    plt.plot(np.arange(0, args.time_step), signal)  \n",
        "    plt.show() "
      ],
      "metadata": {
        "id": "K47jx7jHo5dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signal = torch.norm(mu, p=2, dim=1)**2 \n",
        "signal = signal.cpu().detach().numpy()\n",
        "plt.plot(np.arange(0, args.time_step), signal)  \n",
        "plt.show() "
      ],
      "metadata": {
        "id": "66J0n87x4FlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6lIJoTsQSvA"
      },
      "outputs": [],
      "source": [
        "#mu, loss_holder = main(args, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGJbg3d4zO7e"
      },
      "outputs": [],
      "source": [
        "# torch.diff: second row - first row (then minus sign)# torch.norm: 2-norm of each row# take a squared\n",
        "#signal = 0.5*torch.norm(-torch.diff(mu, dim=0), p=2, dim=1)**2 \n",
        "#signal = signal.cpu().detach().numpy()\n",
        "#plt.plot(np.arange(0, args.time_step-1), signal)  \n",
        "#plt.show()  \n",
        "\n",
        "#print(len(loss_holder))\n",
        "plt.plot(np.arange(0,len(loss_holder)), loss_holder)  \n",
        "plt.show()  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}